{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-97da75c1d52f0687",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Machine Learning with Python - Model Validation and Selection\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous rounds you have implemented machine learning (ML) methods by combining particular choices for\n",
    "\n",
    "* data points, their features and labels, \n",
    "* a hypothesis space (of predictor functions) \n",
    "* and a loss function that measures the quality of a particular predictor function out of the hypothesis space. \n",
    "\n",
    "ML algorithms are optimization methods that try to find (or learn) the best predictor out of the hypothesis space by minimizing the average loss (training error) over some labeled data points (the training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-85d7ef6fc6ff2a13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The code snippet below read in some data points $(x^{(i)},y^{(i)})$, for $i=1,2,\\ldots$, which are characterized by a scalar feature $x^{(i)}$ and a numeric label $y^{(i)}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1aa568825aa58d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAEICAYAAACkmHavAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAdB0lEQVR4nO3df5RcZZ3n8feX0CRBkAgkM+QHJHigwSSaQDMDogmCnMYlSNzDzMgCBlAyyFHEGeIQnV0Y19lhJ5x1h1l/LGfEwC4HVjBGDi5GB8JG5qBjh+jwIwQHESadDGnBjjB0JAnf/aOr2/zoTro61XXrVr9f5+R0171P1f3WU5V8Uvc+9TyRmUiSpHI6qOgCJEnS8BnkkiSVmEEuSVKJGeSSJJWYQS5JUokdXHQBw3H00Ufn9OnTiy5DkqS6WLt27S8zc+JA+0oZ5NOnT6ejo6PoMiRJqouIeGGwfZ5alySpxAxySZJKzCCXJKnESnmNXJKayfbt29m4cSPbtm0ruhQVbNy4cUydOpWWlpYh38cgl6SCbdy4kcMPP5zp06cTEUWXo4JkJi+//DIbN25kxowZQ76fQT6KrFzXybJVG9jU3cPkCeNZ0t7KwrlTii6rZpr9+al5bdu2zRAXEcFRRx1FV1dXVfczyEeJles6WbriCXq27wSgs7uHpSueAGiKsGv256fmZ4gLhvc+cLDbKLFs1Yb+kOvTs30ny1ZtKKii2mr25ydJgzHIR4lN3T1VbS+bZn9+0kg77LDD9rn/F7/4BbNmzarqMS+//HLuu+++Ibcf6jGG8rjLly9n06ZNQz72cHR0dHDttdfus013dzdf/vKXR7QOg3yUmDxhfFXby6bZn5+k6tQjyNva2rj11lv32cYgV80saW9lfMuY3baNbxnDkvbWgiqqrWZ/ftKuVq7r5MybH2bGDd/hzJsfZuW6zpo99muvvcY555zDKaecwuzZs/n2t7/dv2/Hjh0sWrSId77znVx00UW8/vrrAKxdu5b58+dz6qmn0t7ezubNm/d63MHarF27lne9612cccYZfOlLXxqwpszkE5/4BO94xzs4//zz2bJlS/++z3/+85x22mnMmjWLxYsXk5ncd999dHR0cMkllzBnzhx6enoGbLenyy+/nKuvvpr3vve9nHjiiTzwwANA72DEK664gtmzZzN37lxWr14NwCOPPMKCBQsAuOmmm7jyyis566yzOP744/sD/oYbbuC5555jzpw5LFmyhM2bNzNv3jzmzJnDrFmz+MEPflD1azRgB5Xtz6mnnpqq3rce35jv/quHcvqfPZDv/quH8luPbyy6pJpq9uen5vX0008Pue23Ht+YJ/35g3ncnz3Q/+ekP3/wgN/vb3nLWzIzc/v27bl169bMzOzq6sq3v/3t+eabb+bzzz+fQD766KOZmXnFFVfksmXL8o033sgzzjgjt2zZkpmZ99xzT15xxRWZmblo0aK8995799lm9uzZ+cgjj2Rm5vXXX58zZ87cq7ZvfvOb+f73vz937NiRnZ2decQRR+S9996bmZkvv/xyf7tLL70077///szMnD9/fv74xz/u3zdYu10tWrQo29vbc+fOnfnss8/mlClTsqenJ2+55Za8/PLLMzNz/fr1OW3atOzp6cnVq1fn+eefn5mZN954Y55xxhm5bdu27OrqyiOPPDLfeOONfP7553d7Trfcckt+4QtfyMzMHTt25K9//eu96hjo/QB05CCZ6Kj1UWTh3ClNPYK72Z+fBPse2FmL939m8tnPfpY1a9Zw0EEH0dnZyUsvvQTAtGnTOPPMMwG49NJLufXWWznvvPN48sknOffccwHYuXMnxxxzzG6PuWHDhgHbbN26le7ububPnw/AZZddxoMPPrhXTWvWrOHiiy9mzJgxTJ48mbPPPrt/3+rVq/nrv/5rXn/9dV555RVmzpzJBRdcsNdjDLXdH/7hH3LQQQdxwgkncPzxx/PMM8/w6KOP8slPfhKAk046ieOOO45nn312r/uef/75jB07lrFjxzJp0qT+ftvVaaedxpVXXsn27dtZuHAhc+bMGeBVqI5BLkklMtIDO++66y66urpYu3YtLS0tTJ8+vX/GuT2/GhURZCYzZ87kscceG/QxB2vT3d095K9bDdRu27ZtXHPNNXR0dDBt2jRuuummAWfHG2q7gY7T9xyHYuzYsf2/jxkzhh07duzVZt68eaxZs4bvfOc7XHbZZSxZsoSPfOQjQ3r8wXiNXJJKZKQHdm7dupVJkybR0tLC6tWreeGF366e+eKLL/aH8d1338173vMeWltb6erq6t++fft2nnrqqd0ec7A2EyZM4IgjjuDRRx8Fev8TMZB58+Zxzz33sHPnTjZv3tx/jbovjI8++mhee+213UayH3744bz66qv7bbene++9lzfffJPnnnuOn//857S2tjJv3rz+2p599llefPFFWluHNv5m1zoAXnjhBSZNmsRVV13FRz/6UR5//PEhPc6++IlckkpkSXvrbpMfQW0Hdl5yySVccMEFtLW1MWfOHE466aT+fSeffDJ33HEHf/zHf8wJJ5zAxz/+cQ455BDuu+8+rr32WrZu3cqOHTu47rrrmDlzZv/99tXm61//OldeeSWHHnoo7e3tA9b0oQ99iIcffpjZs2dz4okn9p+KnzBhAldddRWzZ89m+vTpnHbaaf336Ru4Nn78eB577LFB2+2ptbWV+fPn89JLL/HVr36VcePGcc0113D11Vcze/ZsDj74YJYvX77bp+99OeqoozjzzDOZNWsWH/jAB5g1axbLli2jpaWFww47jDvvvHNIj7MvMdRTBo2kra0tOzo6ii5Dkmpi/fr1nHzyyUNu73TEI+Pyyy9nwYIFXHTRRYXWMdD7ISLWZmbbQO39RC5JJePATu2qbkEeEbcDC4AtmTlrl+2fBD4B7AC+k5mfqVdNkiT1Wb58edElDEs9B7stB87bdUNEvA+4EHhnZs4EbqljPZLUMMp4mVO1N5z3Qd2CPDPXAK/ssfnjwM2Z+ZtKmy173VGSmty4ceN4+eWXDfNRLivrkY8bN66q+xV9jfxE4L0R8ZfANuD6zPzxQA0jYjGwGODYY4+tX4WSNMKmTp3Kxo0bq16HWs1n3LhxTJ06tar7FB3kBwNvA04HTgO+ERHH5wD/Lc3M24DboHfUel2rlKQR1NLSwowZM4ouQyVV9IQwG4EVlalk/xF4Ezi64JokSSqNooN8JXA2QEScCBwC/LLQiiRJKpF6fv3sbuAs4OiI2AjcCNwO3B4RTwJvAIsGOq0uSZIGVrcgz8yLB9l1ab1qkCSp2RR9al2SJB0Ag1ySpBIzyCVJKrGiv0euBuFqSkNnX0lqJAa5WLmuc7f1jTu7e1i64gkAA2oP9pWkRuOpdbFs1Yb+YOrTs30ny1ZtKKiixmVfSWo0BrnY1N1T1fbRzL6S1GgMcjF5wviqto9m9pWkRmOQiyXtrYxvGbPbtvEtY1jS3lpQRY3LvpLUaBzspv5BWo7E3j/7SlKjiTJObd7W1pYdHR1FlyFJUl1ExNrMbBton6fWJUkqMYNckqQSM8glSSoxg1ySpBKrW5BHxO0RsSUinhxg3/URkRFxdL3qGY1WruvkzJsfZsYN3+HMmx9m5brOokuSJB2gen4iXw6ct+fGiJgGnAu8WMdaRp2+OcI7u3tIfjtHuGEuSeVWtyDPzDXAKwPs+iLwGaB834MrEecIl6TmVOg18oj4INCZmT8dQtvFEdERER1dXV11qK65OEe4JDWnwoI8Ig4FPgf8p6G0z8zbMrMtM9smTpw4ssU1IecIl6TmVOQn8rcDM4CfRsQvgKnA4xHxuwXW1LScI1ySmlNhc61n5hPApL7blTBvy8xfFlVTM3OOcElqTnUL8oi4GzgLODoiNgI3ZubX6nV89Ya5wS1JzaVuQZ6ZF+9n//Q6lSJJUtNwZjdJkkrMIJckqcQMckmSSswglySpxAr7+pk0FCvXdfqVuRqzT6XmYpCrYfUt9NI3R3zfQi+AwTNM9qnUfDy1roblQi+1Z59KzccgV8NyoZfas0+l5mOQq2G50Evt2adS8zHI1bBc6KX27FOp+TjYTQ3LhV5qzz6Vmk9kZtE1VK2trS07OjqKLkOSpLqIiLWZ2TbQPk+tS5JUYga5JEklZpBLklRiBrkkSSVWt1HrEXE7sADYkpmzKtuWARcAbwDPAVdkZne9alJ9OLd382iU17JR6pAaQT0/kS8Hzttj2/eBWZn5TuBZYGkd61Ed9M3t3dndQ/Lbub1XrussujRVqVFey0apQ2oUdQvyzFwDvLLHtu9l5o7KzR8CU+tVj+rDub2bR6O8lo1Sh9QoGuka+ZXAg4PtjIjFEdERER1dXV11LEsHwrm9m0ejvJaNUofUKBoiyCPic8AO4K7B2mTmbZnZlpltEydOrF9xOiDO7d08GuW1bJQ6pEZReJBHxCJ6B8FdkmWcZk775NzezaNRXstGqUNqFIXOtR4R5wF/BszPzNeLrEUjw7m9m0ejvJaNUofUKOo213pE3A2cBRwNvATcSO8o9bHAy5VmP8zMq/f3WM61LkkaTfY113rdPpFn5sUDbP5avY4vSVIzKvwauSRJGj6DXJKkEjPIJUkqsUJHratxOZe1JJWDQa699M1l3TcNZt9c1oBhLkkNxlPr2otzWUtSeRjk2otzWUtSeRjk2otzWUtSeRjk2otzWUtSeTjYTXtxLmtJKg+DXANaOHeKwS1JJeCpdUmSSswglySpxAxySZJKzCCXJKnEDHJJkkqsbkEeEbdHxJaIeHKXbUdGxPcj4meVn2+rVz2SJDWDIQd5RPx9RLzrAI61HDhvj203AA9l5gnAQ5XbkiRpiKr5RP4Z4IsR8fWIOKbaA2XmGuCVPTZfCNxR+f0OYGG1jytJ0mg25CDPzMcz82zgAeC7EXFjRBzo5Nu/k5mbK4+/GZg0WMOIWBwRHRHR0dXVdYCHlSSpOVR1jTwiAtgAfAX4JPCziLhsJArbU2belpltmdk2ceLEehxSkqSGV8018keBTuCLwBTgcuAs4Pci4rZhHv+lvtP0lZ9bhvk4kiSNStXMtX418FRm5h7bPxkR64d5/PuBRcDNlZ/fHubjSJI0KlVzjfzJAUK8z/n7u39E3A08BrRGxMaI+Ci9AX5uRPwMOLdyW5IkDVFNVj/LzJ8Poc3Fg+w6pxY1SJI0GjmzmyRJJWaQS5JUYvs9tR4RrwJ918aj8jMrv2dmvnWEapMkSfux3yDPzMPrUYgkSapeNd8jj4i4NCL+Y+X2tIj4vZErTZIk7U8118i/DJwB/IfK7deAL9W8IkmSNGTVfP3s9zPzlIhYB5CZv4qIQ0aoLkmSNATVfCLfHhFjqAx8i4iJwJsjUpUkSRqSaoL8VuBbwKSI+EvgUeC/jEhVkiRpSIZ8aj0z74qItfTOxBbAwswc7hzrkiSpBqqaojUznwGeGaFaJElSlYYc5BExDrgGeA+918kfBb6SmdtGqDZJkrQf1XwivxN4Ffjbyu2Lgf8F/EGti5IkSUNTTZC3Zua7drm9OiJ+WuuCJEnS0FUzan1dRJzedyMifh/4h9qXJEmShmooi6Y8Qe818RbgIxHxYmXXscDTtSgiIj4NfKxynCeAK7z2LqmRrVzXybJVG9jU3cPkCeNZ0t7KwrlTii5Lo9BQTq0vGMkCImIKcC3wjszsiYhvAB8Glo/kcSVpuFau62Tpiifo2b4TgM7uHpaueALAMFfd7ffUema+0PcH+DXwO8Bxu/yphYOB8RFxMHAosKlGjytJNbds1Yb+EO/Ts30ny1ZtKKgijWbVfP3sY8CngKnAT4DTgceAsw+kgMzsjIhbgBeBHuB7mfm9AY6/GFgMcOyxxx7IISXpgGzq7qlquzSSqhns9ingNOCFzHwfMBfoOtACIuJtwIXADGAy8JaIuHTPdpl5W2a2ZWbbxIkTD/SwkjRskyeMr2q7NJKqCfJtfQPQImJsZZa31hrU8H7g+czsysztwArg3TV4XEkaEUvaWxnfMma3beNbxrCkvRb/JErVqeZ75BsjYgKwEvj7iHiF2lzLfhE4PSIOpffU+jlARw0eV5JGRN+ANketqxFEZlZ/p4j5wFuB71Y+RR9YERF/AfwRsANYB3wsM38zWPu2trbs6DDrJUmjQ0Sszcy2gfYN5Xvkr1JZg3zPXZXtbz2w8iAzbwRuPNDHkSRptNlvkGfm4fUoRJIkVa+awW6SJKnBGOSSJJWYQS5JUokZ5JIklZhBLklSiRnkkiSVmEEuSVKJGeSSJJWYQS5JUokZ5JIklZhBLklSiRnkkiSVmEEuSVKJGeSSJJWYQS5JUontdz3yeoiICcDfAbOABK7MzMeKrUqNZuW6Tpat2sCm7h4mTxjPkvZWFs6dUnRZo4qvgdR4GiLIgb8BvpuZF0XEIcChRRekxrJyXSdLVzxBz/adAHR297B0xRMABkmd+BpIjanwU+sR8VZgHvA1gMx8IzO7i61KjWbZqg39AdKnZ/tOlq3aUFBFo4+vgdSYCg9y4HigC/h6RKyLiL+LiLfs2SgiFkdER0R0dHV11b9KFWpTd09V21V7vgZSY2qEID8YOAX4SmbOBf4NuGHPRpl5W2a2ZWbbxIkT612jCjZ5wviqtqv2fA2kxtQIQb4R2JiZP6rcvo/eYJf6LWlvZXzLmN22jW8Zw5L21oIqGn18DaTGVPhgt8z814j4l4hozcwNwDnA00XXpcbSN5jKEdPF8TWQGlNkZtE1EBFz6P362SHAz4ErMvNXg7Vva2vLjo6OepUnSVKhImJtZrYNtK/wT+QAmfkTYMACJUnS4BrhGrkkSRomg1ySpBIzyCVJKrGGuEYuqXE4n7pULga5pH7Opy6Vj6fWJfVzPnWpfAxySf2cT10qH4NcUj/nU5fKxyCX1M/51KXycbCbpH7Opy6Vj0EuaTcL504xuKUS8dS6JEklZpBLklRiBrkkSSVmkEuSVGIGuSRJJdYwo9YjYgzQAXRm5oKi65EGM1KLirhYiaThaJggBz4FrAfeWnQh0mBGalERFyuRNFwNcWo9IqYC5wN/V3Qt0r6M1KIiLlYiabgaIsiB/w58BnhzsAYRsTgiOiKio6urq36VSbsYqUVFXKxE0nAVHuQRsQDYkplr99UuM2/LzLbMbJs4cWKdqpN2N1KLirhYiaThKjzIgTOBD0bEL4B7gLMj4n8XW5I0sJFaVMTFSiQNV+GD3TJzKbAUICLOAq7PzEsLLUoaxEgtKuJiJZKGq/Agl8pmpBYVcbESScPRUEGemY8AjxRchiRJpdEI18glSdIwGeSSJJWYQS5JUokZ5JIklZhBLklSiRnkkiSVmEEuSVKJGeSSJJWYQS5JUokZ5JIklZhBLklSiRnkkiSVmEEuSVKJGeSSJJWYQS5JUokVvh55REwD7gR+F3gTuC0z/6Yex165rpNlqzawqbuHyRPGs6S9lYVzp9Tj0FJhfN9LzaXwIAd2AH+amY9HxOHA2oj4fmY+PZIHXbmuk6UrnqBn+04AOrt7WLriCQD/UVPT8n0vNZ/CT61n5ubMfLzy+6vAemDE/0VZtmpD/z9mfXq272TZqg0jfWipML7vpeZTeJDvKiKmA3OBHw2wb3FEdERER1dX1wEfa1N3T1XbpWbg+15qPg0T5BFxGPBN4LrM/PWe+zPztsxsy8y2iRMnHvDxJk8YX9V2qRn4vpeaT0MEeUS00Bvid2Xminocc0l7K+Nbxuy2bXzLGJa0t9bj8FIhfN9LzafwwW4REcDXgPWZ+d/qddy+gT2O3tVo4vteaj6RmcUWEPEe4AfAE/R+/Qzgs5n5fwe7T1tbW3Z0dNSjPEmSChcRazOzbaB9hX8iz8xHgSi6DkmSyqghrpFLkqThMcglSSoxg1ySpBIzyCVJKrHCB7tJktQsiliUyCCXJKkGilqUyFPrkiTVQFGLEhnkkiTVQFGLEhnkkiTVQFGLEhnkkiTVQFGLEjnYTZKkGihqUSKDXJKkGlk4d0rdVxP01LokSSVmkEuSVGIGuSRJJWaQS5JUYga5JEklFplZdA1Vi4gu4IWi62gCRwO/LLqIJmA/1ob9WBv2Y200Wj8el5kTB9pRyiBXbURER2a2FV1H2dmPtWE/1ob9WBtl6kdPrUuSVGIGuSRJJWaQj263FV1Ak7Afa8N+rA37sTZK049eI5ckqcT8RC5JUokZ5JIklZhBPgpERGtE/GSXP7+OiOsi4siI+H5E/Kzy821F19roIuLTEfFURDwZEXdHxLiImBERP6r04/+JiEOKrrPRRcSnKn34VERcV9nm+3EIIuL2iNgSEU/usm3Avotet0bEP0fEP0XEKcVV3lgG6cc/qLwn34yItj3aL63044aIaK9/xYMzyEeBzNyQmXMycw5wKvA68C3gBuChzDwBeKhyW4OIiCnAtUBbZs4CxgAfBv4r8MVKP/4K+GhxVTa+iJgFXAX8HvAuYEFEnIDvx6FaDpy3x7bB+u4DwAmVP4uBr9SpxjJYzt79+CTw74E1u26MiHfQ+3d9ZuU+X46IMXWocUgM8tHnHOC5zHwBuBC4o7L9DmBhYVWVx8HA+Ig4GDgU2AycDdxX2W8/7t/JwA8z8/XM3AH8P+BD+H4cksxcA7yyx+bB+u5C4M7s9UNgQkQcU59KG9tA/ZiZ6zNzwwDNLwTuyczfZObzwD/T+x/RhmCQjz4fBu6u/P47mbkZoPJzUmFVlUBmdgK3AC/SG+BbgbVAdyWQADYCU4qpsDSeBOZFxFERcSjw74Bp+H48EIP13RTgX3Zp5/tzeBq6Hw3yUaRy7faDwL1F11JGleuOFwIzgMnAW+g9dbknv9O5D5m5nt7LEd8Hvgv8FNixzztpuGKAbb4/q9fQ/WiQjy4fAB7PzJcqt1/qO81W+bmlsMrK4f3A85nZlZnbgRXAu+k9XXlwpc1UYFNRBZZFZn4tM0/JzHn0nt78Gb4fD8RgfbeR3rMdfXx/Dk9D96NBPrpczG9PqwPcDyyq/L4I+HbdKyqXF4HTI+LQiAh6xxs8DawGLqq0sR+HICImVX4eS+/gorvx/XggBuu7+4GPVEavnw5s7TsFr6rcD3w4IsZGxAx6Bw/+Y8E19XNmt1Gici3yX4DjM3NrZdtRwDeAY+kNqT/IzD0H0WgXEfEXwB/Reyp4HfAxeq+V3QMcWdl2aWb+prAiSyAifgAcBWwH/iQzH/L9ODQRcTdwFr3LbL4E3AisZIC+q/yH83/QO9L6deCKzOwoou5GM0g/vgL8LTAR6AZ+kpntlfafA66k9+/+dZn5YAFlD8gglySpxDy1LklSiRnkkiSVmEEuSVKJGeSSJJWYQS5JUokZ5JIklZhBLklSiRnkUhOKiGsjYn1E3FXl/SZExDUjVZek2nNCGKkJRcQzwAcqSy5Wc7/pwAOV9daruV/Q++/Jm9XcT9KB8xO51GQi4qvA8cD9EfHpiLg0Iv4xIn4SEf8zIsZU2q2MiLUR8VRELK7c/Wbg7ZW2yyJiekQ8uctjXx8RN1V+n1751P9l4HFg2mDH2qO+1RFxbuX3L0TErSPaIVKTM8ilJpOZV9O7MtP76F0m9I+AMzNzDrATuKTS9MrMPBVoA66tzHV+A/BcZs7JzCVDOFwrcGdmzgUO3cexdnUj8LmIuASYC3x6mE9VEnDw/ptIKrFzgFOBH/ee/WY8v13i8tqI+FDl92n0ruj0r1U+/guZ+cMhHKtfZq6pnIr/E+CszNxZ5TEl7cIgl5pbAHdk5tLdNkacRe/66mdk5usR8QgwboD772D3M3d7tvm3/R1rr4IiZgPHAL/MzFeH8iQkDc5T61Jzewi4aJf1v4+MiOOAI4BfVUL8JOD0SvtXgcN3uf9LwKSIOCoixgILhnGsfhFxDHAXcCHwbxHRfuBPURrdDHKpiWXm08CfA9+LiH8Cvk/vp+HvAgdXtv1n4IeV9i8D/xART0bEsszcDnwe+BHwAPDMMI4FQEQcCqwA/jQz11eOe1Ntn7E0+vj1M0mSSsxP5JIklZhBLklSiRnkkiSVmEEuSVKJGeSSJJWYQS5JUokZ5JIkldj/B1/J/qwpp5qIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of labeled data points =  20\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and store data and labels in variables\n",
    "from sklearn import datasets # import datasets fomr sklearn\n",
    "# the library matplotlib.pyplot provides functions for plotting data \n",
    "import matplotlib.pyplot as plt      # import library matplotlib.pyplot as plt\n",
    "# the package \"PolynomialFeatures\" allows to transform a single scalar feature x \n",
    "# into several features given by the powers x^{0},x^{1},x^{2}, ... \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# package \"LinearRegression\" provides methods to fit a linear predictor to \n",
    "# given data points (training data)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "# load the toy dataset \"linnerud\" provide by the \"sklearn\" package\n",
    "linnerud = datasets.load_linnerud()\n",
    "# read in the exercise parameters (nr. of chinups ..) for each athlete\n",
    "Exercise = linnerud['data']\n",
    "# read in the physiological (weight ...) paramters for each athlete\n",
    "Physio = linnerud['target']\n",
    "\n",
    "x = Physio[:,0] \n",
    "# convert Lbs to Kg\n",
    "x = x*0.453 \n",
    "# we use number of chinups as label and store them (for all athletes) in numpy array y\n",
    "y = Exercise[:,0] \n",
    "\n",
    "fig1, axes1 = plt.subplots(1, 1, figsize=(8, 4))\n",
    "axes1.scatter(x, y,label=\"labeled data points\")\n",
    "axes1.set_ylabel('label ' + r'$y$')\n",
    "axes1.set_xlabel('feature ' + r'$x$')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "m_total = y.shape[0] \n",
    "print (\"total number of labeled data points = \", m_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8fd784a728ac00b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let us try to find a good predictor for the label $y$ of a data point using a polynomial $h(x)=w_{1}+w_{2}x+w_{3}x^{2}+\\ldots+w_{r+1}x^{r}$ with some fixed maximum degree $r$ (e.g., $r=4$). The maximum degree $r$ is a measure for the complexity of the hypothesis space given by all polynomials of maximum degree $r$.\n",
    "The larger we choose $r$, the more complex is resulting hypothesis space. In order to find a good predictor we tune the weights $w_{0},w_{1},\\ldots$ to make the average prediction error on a training set as small as possible. Let us pick three data points for the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2601ad49c9617b02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we use some datapoints as the training set \n",
    "\n",
    "fig2, axes2 = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "\n",
    "# choose three indices of data points that will be used \n",
    "# as training set \n",
    "training_set = [m_total-1,m_total-3,m_total-6]\n",
    "# create a dummy numpy array \"dmy\" which will be used to \n",
    "# select data points in the training set and those outside the training set \n",
    "dmy = np.zeros((m_total,1)) \n",
    "# set entries corresponding to training data to 10 \n",
    "dmy[training_set] = 10 \n",
    "\n",
    "x=x.reshape(-1,1)\n",
    "y=y.reshape(-1,1)\n",
    "\n",
    "# choose all data points for which the correpsonding entry of \n",
    "# the numpy array \"dmy\" is larger than 5\n",
    "x_train = x[dmy>5]\n",
    "y_train = y[dmy>5]\n",
    "# choose data points for which corresponding entry of numpy \n",
    "# array \"dmy\" is smaller than 5 (these are indices that are not in training set)\n",
    "x_val = x[dmy<5]\n",
    "y_val = y[dmy<5]\n",
    "# plot labeled data points which are not in training set\n",
    "axes2.scatter(x_val, y_val,label=\"labeled data points\")\n",
    "# plot labeled data points which are in the training set \n",
    "axes2.scatter(x_train, y_train, s=400,marker=r'$\\times$', label=\"training set\")\n",
    "\n",
    "axes2.set_ylabel('label ' + r'$y$')\n",
    "axes2.set_xlabel('feature ' + r'$x$')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-46af00bfd04c2706",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Using three data points (marked by crosses in the above plot) as a training set, we learn (find) the best predictor out the hypothesis space \n",
    "\n",
    "$$ \\mathcal{H} = \\{ h(x) = w_{0}+w_{1}x+w_{2}x^2+w_{3}x^3+w_{4}x^{4} \\mbox{ with tunable weights } w_{0},\\ldots,w_{4} \\in \\mathbb{R} \\}.$$\n",
    "\n",
    "The code snippet below uses the Python library 'scikit-learn' to find weights $w_{0},\\ldots,w_{4}$ such that the average sqaured prediction error $\\big(h\\big(x^{(i)}\\big) - y^{(i)}\\big)^{2}$ on the three training data points is minimal. We denote the predictor obtained by these optimal weights as $h^{(\\rm opt)}(x)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7be1c2de8fb89d1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig3, axes3 = plt.subplots(1, 1, figsize=(8, 4))\n",
    "axes3.scatter(x_val, y_val,label=\"validation set\")\n",
    "\n",
    "training_set = [m_total-1,m_total-3,m_total-6]\n",
    "x_train = x[training_set]\n",
    "y_train = y[training_set]\n",
    "axes3.scatter(x_train, y_train, s=400,marker=r'$\\times$', label=\"training set\")\n",
    "\n",
    "axes3.set_ylabel('label ' + r'$y$')\n",
    "axes3.set_xlabel('feature ' + r'$x$')\n",
    "\n",
    "# choose best predictor out of the hypothesis space given by all \n",
    "# polynomials h(x) = w_1 + w_2*x ... + w5*x^4 of maximum degree 4 \n",
    "\n",
    "poly = PolynomialFeatures(degree = 4) \n",
    "# transform scalar feature x to a feature vector [x^0 x^1 ... x^4]\n",
    "X_poly = poly.fit_transform(x_train.reshape(-1,1)) \n",
    "# we can now use linear regression using the transformed feature vectors \n",
    "lin2 = LinearRegression() \n",
    "# compute optimal weights to minimize training error \n",
    "lin2.fit(X_poly, y_train) \n",
    "\n",
    "# plot the resulting \"optimal\" predictor (having minimum training error) \n",
    "\n",
    "grid_of_x_vals = np.linspace(60, 90, num=100)\n",
    "X_poly_grid = poly.fit_transform(grid_of_x_vals.reshape(-1,1)) \n",
    "axes3.plot(grid_of_x_vals, lin2.predict(X_poly_grid), color = 'red',label=\"optimal predictor\")\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b5608ed9b712d8e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The above figure shows that the learnt predictor $h^{(\\rm opt)}(x)$ (the red curve) almost perfectly fits the training data (organge crosses). The average loss of $h^{(\\rm opt)}(x)$ incurred on the three training data points is essentially zero. However, according to the above figure, the predictor $h^{(\\rm opt)}(x)$ incurs a rather large prediction error on the data points (blue dots) which are not used for training (orange crosses). \n",
    "\n",
    "The key idea of validation is to try out a predictor, which has been found by minimizing its training error, on different data points which are not part of the training data. In the above figure, we can use the prediction error incurred for the data points marked by blue dots to validate the predictor $h^{(\\rm opt)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-49483a7aad5aa158",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Learning goals\n",
    "\n",
    "\n",
    "In this round you will learn a simple but powerful approach for choosing a \"good\" hypothesis space out of a set of alternatives. In particular, you will \n",
    "\n",
    "* learn that the training error is a poor quality measure for a hypothesis space \n",
    "* learn that the validation error is a more useful quality measure for a hypothesis space \n",
    "* learn how to choose between different hypothesis spaces (models) using the validation error\n",
    "* learn about regularization as a soft variant of model selection. \n",
    "\n",
    "## Background Material \n",
    "\n",
    "* [Video lecture](https://www.youtube.com/watch?v=MyBSkmUeIEs) of Prof. Andrew Ng on model validation and selection\n",
    "* [Short video](https://www.youtube.com/watch?v=TIgfjmp-4BA) on K-Fold Cross validation from Udacity\n",
    "* [Video lecture](https://www.youtube.com/watch?v=KvtGD37Rm5I) of Prof. Andrew Ng on regularization\n",
    "* Chapter 2; Chapter 6; Chapter 7 of this [tutorial](https://arxiv.org/abs/1805.05052)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-90c66bf37e8ad022",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "## What is model validation?\n",
    "\n",
    "Assume we want to predict a numeric label (quantitiy of interest) $y \\in \\mathbb{R}$ based on some features $\\mathbf{x}=(x_{1},\\ldots,x_{n}) \\in \\mathbb{R}^{n}$ of a data point. In order to learn a good predictor $h(\\mathbf{x}$, we can use some data points $\\mathbb{X} = \\{ \\big( \\mathbf{x}^{(i)},y^{(i)}\\big)\\}$ for which we have determined the true label value $y^{(i)}$. Each data point in the training data $\\mathbb{X}$ is characterized by features $\\mathbf{x}^{(i)}$ and a label (quantity of interest) $y^{(i)}$. \n",
    "\n",
    "Consider a predictor $h(\\mathbf{x})$ which works extremely well on the dataset $\\mathbb{X}$,\n",
    "\\begin{equation}\n",
    "\\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}}\\big(y^{(i)} - \\underbrace{h(\\mathbf{x}^{(i)})}_{= \\hat{y}^{(i)}}\\big)^{2}\\approx 0.\n",
    "\\end{equation}\n",
    "\n",
    "Even if the predictor $h(\\mathbf{x})$ does exceptionally well on the data set $\\mathbb{X}$, we can not be sure that the method will work well on new data points (different from the data points in $\\mathbb{X}$). \n",
    "This is particularly true for ML methods that allow for highly complicated predictor functions $h(\\mathbf{x})$. Examples of highly complicated predictor functions are linear functions $h(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x} = \\sum_{r=1}^{n} x_{r} w_{r}$ using a large number of features $x_{1},\\ldots,x_{n}$ (the number $n$ of features is a measure of the complexity of the space of linear functions). It can be shown that if the number of features linear predictors on data points with $n$ features allows to perfectly fit any set of $m$ labels $y^{(i)}$ whenever $m \\leq n$. \n",
    "\n",
    "Another example for a vast hypothesis space is given by the set of all predictor functions that can be represented by a given deep neural network structure with billions of adjustable weights (each edge has one weight $w$ that can be tuned). When using an extremely large hypothesis space $\\mathcal{H}$, it is very likely that just by chance one finds a predictor function $h(\\cdot) \\in \\mathcal{H}$ that perfectly fits (reproduces) a given set of labeled data points (unless this dataset is VERY large). \n",
    "\n",
    "ML methods that perform well on training data due to memorization of the training data do not pick up any intrinsic relation between features $\\mathbf{x}$ and label $y$. Such an ML method merely overfits the training data and will not be able to **generalize well** to new data. \n",
    "\n",
    "In order to detect overfitting we need to implement some form of **validation**. The idea behind validation is quite simple: \n",
    "\n",
    "**Split the available labeled data points $\\mathbb{X}$ into two different subsets, a training set $\\mathbb{X}^{(t)}$ of size $m_{t}$ and a validation set $\\mathbb{X}^{(v)}$ of size $m_{v}$.** \n",
    "\n",
    "<img src=\"../../../coursedata/R4_ModelValSel/SplitValTrain.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-714536d18e8f3dff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='splitTestandValidationfunction'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "<b>Demo.</b> Split Data into Training and Validation Set.\n",
    "\n",
    "The code snippet below creates a synthetic dataset of $m$ datapoints $(\\mathbf{x}^{(i)},y^{(i)})$. Each data point is characterized by the feature vector $\\mathbf{x}^{(i)}=\\big(x^{(i)}_{1},\\ldots,x_{n}^{(i)}\\big)^{T} \\in \\mathbb{R}^{n}$ and a numeric label $y^{(i)} \\in \\mathbb{R}$. The feature vectors are stored in the rows of the matrix $\\mathbf{X}\\in \\mathbb{R}^{m \\times n}$. The labels are collected into the vector $\\mathbf{y}=\\big(y^{(1)},\\ldots,y^{(m)}\\big)^{T} \\in \\mathbb{R}^{m}$. \n",
    "\n",
    "The Python library `scikit-learn` provides the function \n",
    "\n",
    "`X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=2)` \n",
    "\n",
    "which can be used to split a dataset into training and validation set. The function reads in the feature vectors in the numpy array `X` of shape ($m,n$) and the labels in the numpy array `y` of shape ($m,1$). \n",
    "\n",
    "The function returns numpy arrays `X_train` of shape ($m_{t},n$), `X_val`of shape ($m_{v},n$), `y_train` of shape ($m_{t},1$) and `y_val` of shape ($m_{v},1$). The input parameter `test_size` specifies the relative size $m_{v}/m$ of the validation set. When using `test_size=0.2`, $20 \\%$ of the original data points are used for the validation set and the and the remaining $80 \\%$ in the training set.\n",
    "\n",
    "[Scikit-learn documentation of train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b8b4f6fb62eef46b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_boston\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split    # Import train_test_split function\n",
    "from sklearn import metrics\n",
    "\n",
    "m = 20    # Number of data points\n",
    "n = 10    # Number of features\n",
    "\n",
    "np.random.seed(4)    # Set random seed for reproducability\n",
    "\n",
    "X = np.random.randn(m,n)    # create feature vectors using random numbers\n",
    "y = np.random.randn(m,1)    # create labels using random numbers \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2)  # Split dataset with 80% training and 20% test\n",
    "\n",
    "plt.rc('legend', fontsize=14)    #  Set font size for legends\n",
    "plt.rc('axes', labelsize=14)    #  Set font size for axis labels\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12,10))    # Create figure with two subplots\n",
    "axes[0].set_title('Scatterplot of the entire dataset and the separate training and validation sets', fontsize=16)\n",
    "\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c='g',marker ='x', s=80, label='original dataset')  # Scatter plot of the original dataset\n",
    "axes[0].legend(loc='best')    # Set legend and set it in the best (automatically determined) position\n",
    "axes[0].set_xlabel(r'feature $x_1$')    # Set the label of the x-axis\n",
    "axes[0].set_ylabel(r'feature $x_2$')    # Set the label of the y-axis\n",
    "\n",
    "axes[1].scatter(X_train[:, 0], X_train[:, 1], c='g',marker ='o', s=80, label='training set')  # Scatter plot of the training set\n",
    "axes[1].scatter(X_val[:, 0], X_val[:, 1], c='brown',marker ='s', s=80, label='validation set')  # Scatter plot of the validation set\n",
    "axes[1].legend(loc='best')    # Set legend and set it in the best (automatically determined) position\n",
    "axes[1].set_xlabel(r'feature $x_1$')    # Set the label of the x-axis\n",
    "axes[1].set_ylabel(r'feature $x_2$')    # Set the label of the y-axis\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d82c01565964839f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The training set $\\mathbb{X}^{(t)}$ is used to learn the optimal predictor $h_{\\rm opt} \\in \\mathcal{H}$ out of the hypothesis space: \n",
    "\\begin{equation} \n",
    "h_{\\rm opt}  = {\\rm argmin}_{h \\in \\mathcal{H}} \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - \\underbrace{h(\\mathbf{x}^{(i)})}_{= \\hat{y}^{(i)}}\\big)^{2}. \n",
    "\\end{equation} \n",
    "The minimum objective value of this optimization problem is the **training error** \n",
    "\\begin{equation}\n",
    "E_{\\rm train} = (1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - h_{\\rm opt}(\\mathbf{x}^{(i)})\\big)^{2}.\n",
    "\\end{equation} \n",
    "Note that the training error $E_{\\rm train}$ measures the performance of the predictor $h_{\\rm opt}$ on the same data points $\\mathbb{X}^{(t)}$ which have been used to tune (learn) $h_{\\rm opt}$. Therefore, the training error $E_{\\rm train}$ is too **optimistic** as an estimate for the average error (or loss) of $h_{\\rm opt}$ on new data points which are different from $\\mathbb{X}^{(t)}$. \n",
    "\n",
    "To estimate the error incurred by $h_{\\rm opt}$ on new data points, we calculate the average loss incurred by $h_{\\rm opt}$ on the validation set $\\mathbb{X}^{(v)}$. This yields the **validation error**\n",
    "\\begin{equation}\n",
    "E_{\\rm val} = (1/m_{v}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(v)}} \\big(y^{(i)} - h_{\\rm opt}(\\mathbf{x}^{(i)})\\big)^{2}. \n",
    "\\end{equation}\n",
    "The validation error $E_{\\rm val}$ is a much better estimate for the average error (or loss) of the predictor $h_{\\rm opt}$. \n",
    "\n",
    "The training error $E_{\\rm train}$ provides a quality measure for the particular predictor $h_{\\rm opt}$. In contrast, the validation error $E_{\\rm val}$ provides a quality measure for the entire hypothesis space $\\mathcal{H}$. Therefore, we can use the validation error for **model selection**. We choose the best hypothesis space $\\mathcal{H}$ out of a set of alternative hypothesis spaces $\\mathcal{H}^{(1)},\\mathcal{H}^{(2)},\\ldots$ according to the corresponding validation errors $E_{\\rm val}(1),E_{\\rm val}(2),\\ldots$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-de358afd2c4fac99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Problem \n",
    "\n",
    "Model validation and selection is best understood by working through a particular example. To this end, we revisit the problem of predicting the grayscale value $y$ of a pixel in an aerial photograph. In **Round 2 - Regression**, we have formalized the grayscale value prediction as an ML problem with\n",
    "\n",
    "1. **data points** which represent pixels in the photograph. Each data point is characterized by features $\\mathbf{x} = (x_{1},\\ldots,x_{n}) \\in \\mathbb{R}^{n}$. Moreover, we define the grayscale value of the pixel as the label $y$ of the data point. \n",
    "\n",
    "2. a **hypothesis space** $\\mathcal{H}$ consisting of predictor functions $h: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ from features $\\mathbf{x} \\in \\mathbb{R}^{n}$ to a predicted price $\\hat{y}=h(\\mathbf{x})\\in \\mathbb{R}$ and \n",
    "\n",
    "3. a **loss function**, such as squared error loss, which measures the quality of a predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3d2949507bb9fa14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='handsondata'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "<p><b>Demo.</b> Loading the Data.</p>\n",
    "    \n",
    "The following code snippet defines a function `X,y = GetFeaturesLabels(m,n)` which reads in the features and labels of pixels which are not corrupted (not fully black). The input parameters are the number `m` of data points and the number `n` of features to be used for each data point. The function returns a matrix $\\mathbf{X}$ and vector $\\mathbf{y}$. \n",
    "\n",
    "The features $\\mathbf{x}^{(i)}$ of data points are stored in the rows of the numpy array `X` (of shape (m,n)) and the corresponding grayscale values $y^{(i)}$ in the numpy array `y` (of shape (m,1)). The two arrays represent the feature matrix $\\mathbf{X} = \\begin{pmatrix} \\mathbf{x}^{(1)} & \\ldots & \\mathbf{x}^{(m)} \\end{pmatrix}^{T}$ and the label vector $\\mathbf{y} = \\big( y^{(1)}, \\ldots, y^{(m)} \\big)^{T}$. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-711d85b7cf810763",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# import \"Pandas\" library/package (and use shorthand \"pd\" for the package) \n",
    "# Pandas provides functions for loading (storing) data from (to) files\n",
    "import pandas as pd  \n",
    "#import cv2\n",
    "from matplotlib import pyplot as plt \n",
    "from IPython.display import display, HTML\n",
    "import numpy as np   \n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def GetFeaturesLabels(m=10, n=10):\n",
    "\n",
    "    corrupted = '/coursedata/R2_Regression/SomePhotoCorrupted.bmp'    # File path of the aerial photo\n",
    "\n",
    "    photo = cv2.imread(corrupted, 0)    # Load image in grayscale (flag=0)    \n",
    "    photo = cv2.resize(photo, (100, 100))    # Resize image to 100x100\n",
    "       \n",
    "    img_width, img_height = photo.shape    # Get image width and height\n",
    "    \n",
    "    # define size of neighorhood which is used to characterize a pixel\n",
    "    wp = 2\n",
    "    hp = 2\n",
    "    \n",
    "    # determine \"uncorroputed pixels\" by finding those pixels with grayscale value larger than 0\n",
    "    good_idx = np.where(photo > 0)\n",
    "    \n",
    "    # avoid pixels close to the boundary since we cannot find neighouring pixels for them \n",
    "    rows = np.clip(good_idx[0], wp, img_height - wp)\n",
    "    cols = np.clip(good_idx[1], hp, img_width - hp)\n",
    "    \n",
    "    # the function cv2.imread() returns an array of grayscale values 0...255 stored as integers \n",
    "    # it will be convenient to convert the integers to floating point numbers \n",
    "    # this allows us to work with grayscale values as real numbers \n",
    "    photo = photo.astype(float)\n",
    "    \n",
    "    sample_size = rows.shape[0]\n",
    "    \n",
    "    # augment image in Data with stripes such that we can also define neighborhoods of border pixels \n",
    "    tmp = np.vstack((np.zeros((wp, img_width)), photo, np.zeros((wp, img_width))))\n",
    "    augmented = np.hstack((np.zeros((2*wp + img_height, hp)), tmp, np.zeros((2*wp + img_height, hp))))\n",
    "\n",
    "    # construct features X for and label vector y for training \n",
    "    x1 = np.zeros((sample_size, 1))\n",
    "    y = np.zeros((sample_size, 1))\n",
    " \n",
    "    for iter_datapoint in range(m): \n",
    "        row_tmp = rows[iter_datapoint] + wp\n",
    "        col_tmp = cols[iter_datapoint] + hp\n",
    "        # the feature of a data point (pixel) is the average gray level of the neighborhoud \n",
    "        x1[iter_datapoint] = np.sum(augmented[(row_tmp-wp):(row_tmp), (col_tmp-hp):(col_tmp + hp + 1)]) / (wp*(2*hp + 1))\n",
    "        y[iter_datapoint] = augmented[row_tmp, col_tmp]\n",
    "\n",
    "    x1 = x1[0:m]\n",
    "    np.random.seed(30)    # Set random seed\n",
    "    # lets add some \"extra features\" here \n",
    "    X = np.hstack((x1, np.random.randn(n,m).T)) \n",
    "    \n",
    "    X = X[:,0:n] \n",
    "    y = y[0:m]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ddfcf8b06137c570",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Linear Predictors \n",
    "\n",
    "To predict the grayscale value $y$ of a pixel based on the first $r$ features (or characteristics) $\\mathbf{x}=(x_{1},\\ldots,x_{r})^{T} \\in \\mathbb{R}^{r}$, we try to find (or learn) a predictor function $h(\\mathbf{x})$ such that $y \\approx h(\\mathbf{x})$. We restrict ourselves to linear predictor functions. Thus, we use the hypothesis space \n",
    "\n",
    "$$ \\mathcal{H}^{(r)} = \\{ h(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x} \\mbox{ with some weight } \\mathbf{w}\\in \\mathbb{R}^{r} \\}.$$ \n",
    "\n",
    "Carefully note that for each value $r\\in \\{1,\\ldots,n\\}$, we obtain a different hypothesis space $\\mathcal{H}^{(r)}$ (or \"model\"). One can verify that these hypothesis spaces are nested in the sense of \n",
    "$$\\mathcal{H}^{(1)} \\subseteq \\mathcal{H}^{(2)} \\subseteq \\mathcal{H}^{(3)} \\ldots .$$\n",
    "\n",
    "For a fixed model parameter $r$, the weight vector $\\mathbf{w} \\in \\mathbb{R}^{r}$ is tuned by minimizing the average squared error loss incurred on the labeled data points in the training set $\\mathbb{X}^{(t)}$: \n",
    "\\begin{align}\\min_{h \\in \\mathcal{H}^{(r)}}  & \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}}  (y^{(i)} - h(\\mathbf{x}^{(i)}) )^{2} \\nonumber \\\\ \n",
    "= \\min_{\\mathbf{w} \\in \\mathbb{R}^{r}} & \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}}  \\big(y^{(i)} -  \\mathbf{w}^{T}\\mathbf{x}^{(i)}  \\big)^{2}.\n",
    "\\end{align}\n",
    "Solving this training problem provides us with optimal choices for weight vector $\\mathbf{w}$. \n",
    "However, we have another design parameter at our disposal: the number $r$ of features! While in our data base each pixel is characterized by $n$ features, we are free to use fewer e.g. only the first $r \\leq n$ of these features. \n",
    "\n",
    "What is the best choice for $r$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0399c961c185f2f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Wrong Way \n",
    "\n",
    "Let us try out each hypothesis space $\\mathcal{H}^{(r)}$ on the training data $\\mathbb{X}^{(t)}$. For each $r=1,\\ldots,h,$ we learn the optimal predictor $h_{\\rm opt}^{(r)} \\in \\mathcal{H}^{(r)}$ by minimizing the average loss on the training set: \n",
    "\\begin{align} \n",
    "h_{\\rm opt}^{(r)} & = {\\rm argmin}_{h \\in \\mathcal{H}^{(r)}} (1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big( y^{(i)}- h(\\mathbf{x}^{(i)}) \\big)^{2}. \n",
    "\\end{align} \n",
    "The corresponding training error is     \n",
    "\\begin{align} \n",
    "E_{\\rm train}(r) & = (1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big( y^{(i)}- h_{\\rm opt}^{(r)}(\\mathbf{x}^{(i)}) \\big)^{2} \\nonumber \\\\ \n",
    "& = \\min_{h \\in \\mathcal{H}^{(r)}} (1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big( y^{(i)}- h(\\mathbf{x}^{(i)}) \\big)^{2} \\nonumber \\\\ \n",
    "& = \\min_{\\mathbf{w} \\in \\mathbb{R}^{r}} (1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big( y^{(i)}- \\mathbf{w}^{T} \\mathbf{x}^{(i)}  \\big)^{2} \\nonumber \\\\ \n",
    "& = \\min_{\\mathbf{w} \\in \\mathbb{R}^{r}} (1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big( y^{(i)}- \\sum_{s=1}^{r}w_{s} x_{s}^{(i)}  \\big)^{2} \n",
    "\\end{align} \n",
    "It is tempting to choose the number $r$ of features according to the smallest training error $E_{\\rm train}(r)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0aaf4cb2c4109eb2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='trainModel'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "<p><b>Demo.</b> Varying Number of Features </p>\n",
    "    \n",
    "The following code snippet computes the training error E(r) for each choice for the number r of features. For each particular value $r=1,\\ldots,n$, the best linear predictor $h(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$ is using the Python function `LinearRegression.fit()`\n",
    "\n",
    "[Documentation of the LinearRegression class in scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) \n",
    "\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-acd5c9243afcd36f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b2ec19f38af1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m                        \u001b[1;31m# maximum number of features used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGetFeaturesLabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# read in m data points using n features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mlinreg_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# vector for storing the training error of LinearRegresion.fit() for each r\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-981939930ab4>\u001b[0m in \u001b[0;36mGetFeaturesLabels\u001b[1;34m(m, n)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mcorrupted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/coursedata/R2_Regression/SomePhotoCorrupted.bmp'\u001b[0m    \u001b[1;31m# File path of the aerial photo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mphoto\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrupted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# Load image in grayscale (flag=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mphoto\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphoto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# Resize image to 100x100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "m = 10                        # we use the first m=20 data points (pixels) from the aerial photo \n",
    "n = 10                        # maximum number of features used \n",
    "\n",
    "X,y = GetFeaturesLabels(m,n)  # read in m data points using n features \n",
    "linreg_error = np.zeros(n)    # vector for storing the training error of LinearRegresion.fit() for each r\n",
    "\n",
    "for r_minus_1 in range(n):    # loop over number of features r (minus 1)\n",
    "    reg = LinearRegression(fit_intercept=False)    # create an object for linear predictors\n",
    "    reg = reg.fit(X[:,:(r_minus_1 + 1)], y)    # find best linear predictor (minimize training error)\n",
    "    pred = reg.predict(X[:,:(r_minus_1 + 1)])    # compute predictions of best predictors \n",
    "    linreg_error[r_minus_1] = mean_squared_error(y, pred)    # compute training error \n",
    "\n",
    "plot_x = np.linspace(1, n, n, endpoint=True)    # plot_x contains grid points for x-axis (1,...,n)\n",
    "\n",
    "# Plot training error E(r) as a function of feature number r\n",
    "plt.rc('legend', fontsize=14)    #  Set font size for legends\n",
    "plt.rc('axes', labelsize=14)    #  Set font size for axis labels\n",
    "plt.figure(figsize=(10,6))    # Set figure size\n",
    "plt.plot(plot_x, linreg_error, label='$E(r)$', color='red')\n",
    "plt.xlabel('# of features $r$')\n",
    "plt.ylabel('training error $E(r)$')\n",
    "plt.title('training error vs number of features', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4fba2c040fb160f3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Let's Interpret the Results!\n",
    "\n",
    "Based on the above plot, we could argue that we should choose the linear model with $r=10$ features since this yields the lowest training error $E(r)$. **This reasoning is incorrect** since our ultimate goal is to find a predictor for new pixels for which we do not know the grayscale values (e.g. corrupted pixels). Our goal is not to accurately reproduce the grayscale values of pixels for which we already know these values! \n",
    "\n",
    "Using the training error $E_{\\rm train}(r)$ to assess the quality of the predictor $h_{\\rm opt}^{(r)}$ is misleading since $h_{\\rm opt}^{(r)}$ is based on the weight vector $\\mathbf{w}$ and intercept that is perfectly tuned to the training data $\\mathbb{X}^{(t)}$. Also, the more features (larger $r$) we use, the better we will be able to fit the training data $\\mathbb{X}^{(t)}$ (obtain smaller training error). However, this does not necessarily lead to better performance on new data. A complex model with too many features (large $r$) might only fit the training data very well, and generalize poorly to new data.\n",
    "\n",
    "Consider the case of $r=m_{\\rm train}$, i.e., the number of features is the same as the number of labeled data points in the training set. Under very mild conditions it can be shown that in this case there always exists a linear predictor $h(\\mathbf{x})=\\mathbf{w}^{T} \\mathbf{x}$ such that $y^{(i)} = h(\\mathbf{x}^{(i)})$, i.e., the training error is exactly zero (see Chapter 7.1 of the coursebook)! \n",
    "A better way to evaluate the quality of a predictor is presented next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8e508ee65304e99f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##  The Right Way\n",
    "\n",
    "The training error $E_{\\rm train}(r)$ is a bad measure for the quality of a hypothesis space $\\mathcal{H}^{(r)}$ since it will always favor larger spaces (larger number $r$ of features). A more useful measure for the quality of a hypothesis space $\\mathcal{H}^{(r)}$ is the validation error \n",
    "\\begin{equation}\n",
    "E_{\\rm val}(r) = (1/m_{v}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(v)}} \\big(y^{(i)} - h^{(r)}_{\\rm opt}(\\mathbf{x}^{(i)})\\big)^{2}. \n",
    "\\end{equation} \n",
    "Here, the predictor $h_{\\rm opt}$ is obtained by minimizing the training error over all linear predictors using $r$ features: \n",
    "\\begin{equation}\n",
    " h^{(r)}_{\\rm opt} = {\\rm argmin}_{h \\in \\mathcal{H}^{(r)}} (1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - h(\\mathbf{x}^{(i)})\\big)^{2}.\n",
    "\\end{equation}\n",
    "Since each predictor function $h \\in \\mathcal{H}^{(r)}$ is given by $h(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$ we can find the optimal predictor via the optimum weight and intercept \n",
    "\\begin{equation}\n",
    "\\mathbf{w}_{\\rm opt} = {\\rm argmin}_{\\mathbf{w}\\in \\mathbb{R}^{r}} (1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - \\mathbf{w}^{T} \\mathbf{x}^{(i)} \\big)^{2}. \n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a7b319a106cbd05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='splitTestandValidationfunction'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Generate Training and Validation Set.\n",
    "   \n",
    "Use the `scikit-learn` library function `train_test_split()` to split the data points obtained from the function `GetFeaturesLabels` into a training and validation set. The function should be used with the choice `random_state=2` and `test_size=0.2`. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9cc67382efb4ce19",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split    # Import train_test_split function\n",
    "\n",
    "m = 20                        # we use the first m=20 data points (pixels) from the aerial photo\n",
    "n = 10                        # maximum number of features used \n",
    "\n",
    "X, y = GetFeaturesLabels(m,n)    # read in m data points using n features \n",
    "\n",
    "### STUDENT TASK ###\n",
    "# Compute the training and validation sets\n",
    "# X_train, X_val, y_train, y_val = ...\n",
    "### BEGIN SOLUTION\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2)  # 80% training and 20% test\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2fbabdcefb77f271",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check dimensions of train and validation vectors\n",
    "assert len(X_train) == 16, \"The 'x_train' vector has the wrong length\"\n",
    "assert len(y_train) == 16, \"The 'y_train' vector has the wrong length\"\n",
    "assert len(X_val) == 4,   \"The 'x_val' vector has the wrong length\"\n",
    "assert len(y_val) == 4, \"The 'y_val' vector has the wrong length\"\n",
    "print('Sanity checks passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-428dadef568d5aef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='trainValErrorsfunction'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Compute Training and Validation Error. \n",
    "\n",
    "**(1)** Complete the function `get_train_val_errors(X_train, X_val, y_train, y_val, n_features)` that returns the training error and validation error for each choice of $r=1,\\ldots,n$. Please use `fit_intercept=True` The training errors should be stored in a numpy array `err_train` of shape (n,1) and the validation errors should be stored in the numpy array `err_val` of shape (n,1). The first entries of `err_train` and `err_val` should be $E_{\\rm train}(1)$ and $E_{\\rm val}(1)$. \n",
    "\n",
    "**(2)** Complete the function `get_best_model(err_val)`, that takes as input the validation errors `err_val` for each number of features $r=1,\\ldots,n$ and returns the optimum number $\\hat{r}$ of features (such that the validation error is smallest). \n",
    "\n",
    "Hint: you can determine the index of the smallest entry in a numpy array using `np.argmin()` ([see documentation](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.argmin.html)).\n",
    "\n",
    "**IMPORTANT!**: Remember that indexing for numpy arrays starts with index 0. However, we start with model size $r=1$. Thus, you need to add 1 to the index that you get by using `np.argmin()`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7791084dee96529b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_train_val_errors(X_train, X_val, y_train, y_val, n_features):  \n",
    "    err_train = np.zeros((n,1))    # Array for storing training errors\n",
    "    err_val = np.zeros((n,1))    # Array for storing validation errors\n",
    "    \n",
    "    for r_minus_1 in range(n):    # Loop over the number of features r (minus one)\n",
    "        ### STUDENT TASK ###\n",
    "        ### BEGIN SOLUTION\n",
    "        lin_reg = LinearRegression(fit_intercept=True)\n",
    "        lin_reg = lin_reg.fit(X_train[:,:(r_minus_1+1)], y_train)\n",
    "        w_opt = lin_reg.coef_\n",
    "        y_pred_train = lin_reg.predict(X_train[:,:(r_minus_1+1)])\n",
    "        err_train[r_minus_1] = mean_squared_error(y_train, y_pred_train)\n",
    "        y_pred_val = lin_reg.predict(X_val[:,:(r_minus_1+1)])\n",
    "        err_val[r_minus_1] = mean_squared_error(y_val, y_pred_val)\n",
    "        ### END SOLUTION\n",
    "    return err_train, err_val\n",
    "\n",
    "def get_best_model(err_val):\n",
    "    # best_model = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    best_model = np.argmin(err_val) + 1\n",
    "    ### END SOLUTION\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c1430b345ba97bfc",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate training and validation errors using ´get_train_val_errors´\n",
    "err_train, err_val = get_train_val_errors(X_train, X_val, y_train, y_val, n)\n",
    "\n",
    "# Perform some sanity checks on the results\n",
    "assert err_train.shape == (n,1), \"numpy array err_train has wrong shape\"\n",
    "assert err_val.shape == (n,1), \"numpy array err_val has wrong shape\"\n",
    "print('Sanity checks passed!')\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "def test_get_train_val_errors(X_train, X_val, y_train, y_val, n_features):  \n",
    "    err_train = np.zeros([n,1]) # Array for storing training errors\n",
    "    err_val = np.zeros([n,1]) # Array for storing validation errors\n",
    "    \n",
    "    for r_minus_1 in range(n):\n",
    "        lin_reg = LinearRegression(fit_intercept=True)\n",
    "        lin_reg = lin_reg.fit(X_train[:,:(r_minus_1+1)], y_train)\n",
    "        w_opt = lin_reg.coef_\n",
    "        y_pred_train = lin_reg.predict(X_train[:,:(r_minus_1+1)])\n",
    "        err_train[r_minus_1] = mean_squared_error(y_train, y_pred_train)\n",
    "        y_pred_val = lin_reg.predict(X_val[:,:(r_minus_1+1)])\n",
    "        err_val[r_minus_1] = mean_squared_error(y_val, y_pred_val)\n",
    "    return err_train, err_val\n",
    "\n",
    "t_err_train, t_err_val = test_get_train_val_errors(X_train, X_val, y_train, y_val, n)\n",
    "\n",
    "np.testing.assert_allclose(t_err_train.reshape(-1,1), err_train.reshape(-1,1), atol=1e-3, err_msg='get_train_val_errors is not correctly calculating the training errors')\n",
    "np.testing.assert_allclose(t_err_val.reshape(-1,1), err_val.reshape(-1,1), atol=1e-3, err_msg='get_train_val_errors is not correctly calculating the validation errors')\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b6e6c7e09a33407f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the best model using `get_best_model`\n",
    "best_model = get_best_model(err_val)\n",
    "\n",
    "# Print the best model\n",
    "print('The best model is obtained for r={}'.format(best_model))\n",
    "\n",
    "# Perform some sanity checks on the result\n",
    "assert best_model != None, \"Please choose a value between 1 and n \"\n",
    "assert best_model <= n, \"The values should be less than n\"\n",
    "assert best_model > 0, \"The values should be more than 0\"\n",
    "print('Sanity checks passed!')\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "def test_get_train_val_errors(X_train, X_val, y_train, y_val, n_features):  \n",
    "    err_train = np.zeros([n,1]) # Array for storing training errors\n",
    "    err_val = np.zeros([n,1]) # Array for storing validation errors\n",
    "    \n",
    "    for r_minus_1 in range(n):\n",
    "        lin_reg = LinearRegression(fit_intercept=True)\n",
    "        lin_reg = lin_reg.fit(X_train[:,:(r_minus_1+1)], y_train)\n",
    "        w_opt = lin_reg.coef_\n",
    "        y_pred_train = lin_reg.predict(X_train[:,:(r_minus_1+1)])\n",
    "        err_train[r_minus_1] = mean_squared_error(y_train, y_pred_train)\n",
    "        y_pred_val = lin_reg.predict(X_val[:,:(r_minus_1+1)])\n",
    "        err_val[r_minus_1] = mean_squared_error(y_val, y_pred_val)\n",
    "    return err_train, err_val\n",
    "\n",
    "t_err_train, t_err_val = test_get_train_val_errors(X_train, X_val, y_train, y_val, n)\n",
    "t_best_model = np.argmin(t_err_val) + 1\n",
    "\n",
    "assert best_model == t_best_model\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-802d3729eb036c9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, we plot the training and validation errors from the previous task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-64473b41dbc92dfa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation errors for the different number of features r\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1, n + 1), err_train, color='black', label=r'$E_{\\rm train}(r)$', marker='o')  # Plot training error\n",
    "plt.plot(range(1, n + 1), err_val, color='red', label=r'$E_{\\rm val}(r)$', marker='x')  # Plot validation error\n",
    "\n",
    "plt.title('Training and validation error for different number of features', fontsize=16)    # Set title\n",
    "plt.ylabel('Empirical error')    # Set label for y-axis\n",
    "plt.xlabel('r features')    # Set label for x-axis\n",
    "plt.xticks(range(1, n + 1))  # Set the tick labels on the x-axis to be 1,...,n\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-61ecf20008a16c4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We see that the training error is monotonically decreasing with increasing number $r$ of features used in the linear predictor $h(\\mathbf{x}) = w_{1}x_{1}+\\ldots+w_{r}x_{r}$. However, the validation error is first decreasing but then significantly increasing for larger values of $r$. Note that the validation error is an estimate for the prediction error on new data points. For large values of $r$, the training error can be highly misleading as a measure for prediction error on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a417d9495eb3f40e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## K-fold Cross-Validation\n",
    "\n",
    "In general, there is no unique optimal way of splitting a data set into a training and validation set. The precise choice of how to divide data points into the training and validation set and also their relative size (80/20, 50/50 ...) has to be considered case-by-case for the application at hand. \n",
    "\n",
    "To get more guidance on how to split the data, one typically needs to have additional knowledge about the statistical properties of the data generating process. An accurate probabilistic model for the data points allows determining optimal split ratios between the training and validation set. That said, probabilistic (generative) models for the observed data points is beyond the scope of this course. \n",
    "\n",
    "Using only a single split of the data into training and validation set bears the risk of being extremely \"unlucky\". The single split might result in a highly non-typical validation set such that the validation error is not reliable as a measure for the average loss on new data. $K$-fold cross-validation is a straightforward extension of the \"single-split approach\" making it more robust. \n",
    "\n",
    "$K$-fold cross-validation randomly splits the data into $K$ equal-sized subsets (\"folds\"). It then executes $K$ rounds, each round corresponding to one of the $K$ folds. In the $k$th round, the $k$th fold is used as the validation set and the remaining $K-1$ folds are used as the training set. The validation errors obtained during each fold are then averaged to obtain the final validation error. \n",
    "\n",
    "As an example, a diagram of  5-fold cross-validation is depicted below. For each round, the fold which is used as the validation set is indicated by \"test\". \n",
    "\n",
    "![Components](../../../coursedata/R4_ModelValSel/cross_validation_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-de6d839671d97847",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='kfold'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "<p><b>Student task.</b> Splitting data into K-folds in sklearn.</p>\n",
    "    \n",
    "The code snippet below shows how to use a `KFold` object in scikit-learn to iterate through `K` train/validation splits of the dataset `X`.\n",
    "    \n",
    "On initialization the `KFold` object is given the number of folds `K` as an argument. The Python [generator function](https://docs.python.org/3.8/glossary.html#term-generator) `KFold.split(X)` can then be used to iterate through the pairs of training and validation indices. \n",
    "\n",
    "For an array `idx` of indices, the data points in X corresponding to these indices can be obtained by `X[idx,:]`. We can use this to obtain the training and validation sets given the indices of the datapoints in the respective sets.\n",
    "\n",
    "For more information, see the scikit-learn [documentation of KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html).\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-045f017b3f6cb246",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Import KFold class from scikitlearn library\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "K=5    # Specify the number of folds of split data into\n",
    "kf = KFold(n_splits=K, shuffle=False)    # Create a KFold object with 'K' splits\n",
    "\n",
    "# For all splits, print the validation and training indices\n",
    "iteration = 0\n",
    "for train_indices, test_indices in kf.split(X):\n",
    "    iteration += 1\n",
    "    X_train = X[train_indices,:]    # Get the training set    \n",
    "    X_val = X[test_indices,:]    # Get the validation set\n",
    "    print('Iteration {}:'.format(iteration))\n",
    "    print('Indices for validation set:', test_indices)\n",
    "    print('Indices for training set:', train_indices)\n",
    "    print('X_val shape: {}, X_train shape: {} \\n'.format(X_val.shape, X_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-13d40bbc019a9337",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='kfold'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<p><b>Student task.</b> K-Fold Cross Validation.</p>\n",
    "    \n",
    "Your task is to fill in the missing parts of the code section below, in which the average training and validation errors of a linear model (with features $x_1,x_2$) is calculated using K-fold cross-validation, for different numbers `K` of folds.\n",
    "    \n",
    "For each `K` in `K_list`, the code should:\n",
    "\n",
    "1. Create a `KFold` object using `K` folds (using `shuffle=False` as in the demo)\n",
    "    \n",
    "2. Iterate over the `K` pairs of train and test indices, and calculate and store the training and validation errors of the linear model for each split in `train_errors_per_cv_iteration` and `test_errors_per_cv_iteration` respectively\n",
    "    \n",
    "3. Calculate the average training- and validation error and append these to `err_train_folds` and `err_val_folds`\n",
    "\n",
    "\n",
    "\n",
    "For more information, see the scikit-learn [documentation of KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html).\n",
    "    \n",
    "\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eb92bc591606de2f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "m = 600    # we use the first m=600 data points (pixels) from the aerial photo\n",
    "n = 2    # number of features used \n",
    "\n",
    "X,y = GetFeaturesLabels(m,n)  # read in m data points with n features \n",
    "\n",
    "K_list = list(range(2,11))   # List of K:s to use (2,...,10)\n",
    "err_train_folds = []  # List to store the average training error over K folds for each K in K_list\n",
    "err_val_folds = []  # List to store the average validation error over K folds for each K in K_list\n",
    "\n",
    "# Loop over all values of K in K_list\n",
    "for K in K_list:\n",
    "\n",
    "    train_errors_per_cv_iteration = []    # List to store the training errors of the 'K' train/test splits (for current K)\n",
    "    test_errors_per_cv_iteration = []    # List to store the test errors of the 'K' train/test splits (for current K)\n",
    "    \n",
    "    ### STUDENT TASK ###\n",
    "    ### BEGIN SOLUTION\n",
    "    kf = KFold(n_splits=K, shuffle=False)    # Create a KFold object with 'K' splits\n",
    "\n",
    "    # Iterate over the 'K' different train/test splits in kf   \n",
    "    for train_indices, test_indices in kf.split(X):\n",
    "        reg = LinearRegression(fit_intercept=False)    # Create new linear regression model\n",
    "        reg = reg.fit(X[train_indices,:], y[train_indices])    # Fit the model on the current training set\n",
    "        y_pred_train = reg.predict(X[train_indices,:])    # Calculate the predicted labels of the current training set\n",
    "        train_errors_per_cv_iteration.append(mean_squared_error(y[train_indices], y_pred_train))    # Add the training error to list of errors\n",
    "        y_pred_val = reg.predict(X[test_indices,:])    # Calculate the predicted labels of the current test set\n",
    "        test_errors_per_cv_iteration.append(mean_squared_error(y[test_indices], y_pred_val))    # Add the test error to list of errors\n",
    "            \n",
    "    err_train = np.mean(train_errors_per_cv_iteration)    # compute the mean of round-wise training errors\n",
    "    err_val = np.mean(test_errors_per_cv_iteration)    # compute the mean of round-wise validation errors\n",
    "    err_train_folds.append(err_train)\n",
    "    err_val_folds.append(err_val)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "print('Training errors for each K:')\n",
    "print(err_train_folds, '\\n')\n",
    "print('Validation error for each K:')\n",
    "print(err_val_folds, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2d51040a6c736472",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform some sanity checks on the result\n",
    "assert len(err_train_folds) == len(K_list), \"err_train_folds is of the wrong length!\"\n",
    "assert len(err_val_folds) == len(K_list), \"err_val_folds is of the wrong length!\"\n",
    "assert (err_val_folds[0] - err_train_folds[0]) > (err_val_folds[8] - err_train_folds[8])\n",
    "print('Sanity checks passed!')\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "def t_kfold():\n",
    "    K_list = list(range(2,11))   # List of K:s to use (2,...,10)\n",
    "    err_train_folds = []  # List to store the average training error over K folds for each K in K_list\n",
    "    err_val_folds = []  # List to store the average validation error over K folds for each K in K_list\n",
    "\n",
    "    # Loop over all candidate values of K\n",
    "    for K in K_list:\n",
    "\n",
    "        train_errors_per_cv_iteration = []    # List to store the training errors of the 'K' train/test splits (for current K)\n",
    "        test_errors_per_cv_iteration = []    # List to store the test errors of the 'K' train/test splits (for current K)\n",
    "\n",
    "        kf = KFold(n_splits=K, shuffle=False)    # Create a KFold object with 'K' splits\n",
    "\n",
    "        # Iterate over the 'K' different train/test splits in kf   \n",
    "        for train_indices, test_indices in kf.split(X):\n",
    "            reg = LinearRegression(fit_intercept=True)    # Create new linear regression model\n",
    "            reg = reg.fit(X[train_indices,:], y[train_indices])    # Fit the model on the current training set\n",
    "            y_pred_train = reg.predict(X[train_indices,:])    # Calculate the predicted labels of the current training set\n",
    "            train_errors_per_cv_iteration.append(mean_squared_error(y[train_indices], y_pred_train))    # Add the training error to list of errors\n",
    "            y_pred_val = reg.predict(X[test_indices,:])    # Calculate the predicted labels of the current test set\n",
    "            test_errors_per_cv_iteration.append(mean_squared_error(y[test_indices], y_pred_val))    # Add the test error to list of errors\n",
    "\n",
    "        err_train = np.mean(train_errors_per_cv_iteration)    # compute the mean of round-wise training errors\n",
    "        err_val = np.mean(test_errors_per_cv_iteration)    # compute the mean of round-wise validation errors\n",
    "        err_train_folds.append(err_train)\n",
    "        err_val_folds.append(err_val)\n",
    "        \n",
    "    return err_train_folds, err_val_folds\n",
    "\n",
    "t_err_train_folds, t_err_val_folds = t_kfold()\n",
    "np.testing.assert_allclose(t_err_train_folds, err_train_folds, atol=1e-3, err_msg=\"The training errors for different values of K are incorrect\")\n",
    "np.testing.assert_allclose(t_err_val_folds, err_val_folds, atol=1e-3, err_msg=\"The validation errors for different values of K are incorrect\")\n",
    "### END HIDDEN TESTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e74e59f6de5faba1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation errors as a function of the number K of folds\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(K_list, err_train_folds)\n",
    "plt.plot(K_list, err_val_folds)\n",
    "plt.xlabel('K folds')\n",
    "plt.ylabel(\"Error\"\n",
    "          )\n",
    "plt.title(\"Average training and validation errors vs. number of folds\")\n",
    "plt.legend(['Training error', 'Validation error'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-12f1478400587745",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If the student task has been solved correctly, the figure above should show a trend of increasing training error and decreasing validation error when increasing the number of folds $K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a451b1c380ed82da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##  Regularization\n",
    "\n",
    "Consider a ML method based on a large hypothesis space such as linear predictors using many features or polynomials with a large degree. Large hypothesis spaces typically contain complex predictors that achieve very low training errors by overfitting the data. Thus, if we search for the optimal predictor in this hypothesis space (i.e train our model) by minimizing the training error, we will obtain a predictor that overfits the training data and  generalizes poorly to data that is not in the training set.\n",
    "\n",
    "One solution to prevent overfitting is to choose a model out of a selection of candidate models based on the validation error, like we did in the student task \"Compute Training and Validation Error\". By using the validation error as the selection criteria, we are able to select the model that performs best on new data. While this approach is useful, it can be very difficult to implement in settings where the number of feasible hypothesis spaces is very large.\n",
    "\n",
    "**Regularization** is a more sophisticated approach to preventing overfitting, and is based on the idea of **estimating the expected increase of the validation error (relative to the training error) incurred by more complex predictors**. When applying regularization, a large hypothesis space is used in conjunction with a loss function that penalizes the complexity of the predictor. If we recall that the optimal predictor is found by minimizing the loss function, it is clear that by choosing a loss function that penalizes complexity, the optimal predictor will less complex. In practice, the penalization is done by adding a **regularization term** $\\mathcal{R}(h)$ to the training error: \n",
    "\n",
    "\\begin{equation}\n",
    "h^{(\\lambda)}_{\\rm opt}  = {\\rm argmin}_{h \\in \\mathcal{H}} \\underbrace{\\underbrace{(1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - h(\\mathbf{x}^{(i)}) \\big)^{2}}_{\\mbox{ training error}} + \\underbrace{\\alpha \\mathcal{R}(h)}_{\\mbox{anticipated increase of error (loss) on new data}}}_{\\mbox{ estimate (approximation) of validation error }}.  \n",
    "\\end{equation}\n",
    "\n",
    "The regularization term $\\mathcal{R}(h)$ quantifies the anticipated increase in the validation error (compared to the training error) due to the \"complexity\" (e.g. the number of features used in a linear predictor) of a particular predictor. In a nutshell, the regularization term penalizes the use of more complex predictors and therefore favors \"simpler\" predictor functions. The precise meaning of \"complexity\" or \"simpler\" is determined by the (design) choice for the regularization term $\\mathcal{R}(h)$. \n",
    "\n",
    "Two widely used choices for measuring the complexity of linear predictors $h(\\mathbf{x}) = \\mathbf{w}^{T}\\mathbf{x}$ is the squared Euclidean norm $\\mathcal{R}(h) = \\|\\mathbf{w}\\|^{2}_{2}=\\sum_{r=1}^{n} w_{r}^{2}$ or the $\\ell_{1}$ norm $\\mathcal{R}(h) = \\|\\mathbf{w}\\|_{1}=\\sum_{r=1}^{n} |w_{r}|$. \n",
    "\n",
    "The regularization parameter $\\alpha$ **offers a trade-off between the prediction error (training error) incurred on the training data and the complexity of a predictor**. The larger we choose $\\alpha$, the more emphasis is put on obtaining \"simple\" predictor functions. Using very small values for $\\alpha$ prefers predictor functions which achieve a small training error (at the expense of being a more complicated function).\n",
    "\n",
    "In order to actually implement regularization, we need to \n",
    "- choose (define) the function $\\mathcal{R}(h)$ that quantifies some notion of complexity of a predictor function $h \\in \\mathcal{H}^{(n)}$. \n",
    "- choose the value of the regularization parameter $\\alpha$. \n",
    "\n",
    "A principled approach to these choices is to assume a probabilistic model for how the data points are generated. It is then possible to relate optimal choices for the function $\\mathcal{R}(h)$ and the value $\\alpha$ to the parameters of the probability distribution of the data points. However, probabilistic modelling in machine learning is beyond the scope of this course. \n",
    "\n",
    "Instead of probabilistic modelling, we can use again the concept of validation sets to find good choices for the regularization function and parameter. In particular, \n",
    "- we first specify a set of different choices for the function $\\mathcal{R}(h)$ and regularization parameter value $\\alpha$, \n",
    "- for each choice for $\\alpha$ and $\\mathcal{R}(h)$, we learn a predictor that minimizes the regularized training error \n",
    "\n",
    "\\begin{equation}\n",
    "h^{(\\alpha)}_{\\rm opt}  = {\\rm argmin}_{h \\in \\mathcal{H}} (1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - h(\\mathbf{x}^{(i)}) \\big)^{2} + \\alpha \\mathcal{R}(h).    \n",
    "\\end{equation}\n",
    "- evaluate the resulting predictor $h^{(\\alpha)}_{\\rm opt}$ by computing the validation error\n",
    "\\begin{equation}\n",
    "E_{\\rm val}^{(\\alpha)} = (1/m_{\\rm v}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(v)}} \\big(y^{(i)} - h^{(\\alpha)}_{\\rm opt}(\\mathbf{x}^{(i)})\\big)^{2}.\n",
    "\\end{equation} \n",
    "\n",
    "We then use the regularization measure $\\mathcal{R}(h)$ and the value for $\\alpha$ with smallest validation error $E_{\\rm val}^{(\\alpha)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a697f48e6cf7e933",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='ridgeReg'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "<p><b>Demo.</b> Ridge Regression. </p>\n",
    "\n",
    "Ridge regression learns a linear predictor functions $h^{(\\mathbf{w})}(\\mathbf{x}) =\\mathbf{w}^{T} \\mathbf{x}$ by minimizing the sum of training error and the scaled regularization term $\\mathcal{R}(h)=\\|\\mathbf{w}\\|_{2}^{2}$. A ridge regression model can be fitted to a dataset with scikit-learn by using the function `Ridge.fit()`. After fitting the model, the optimal weight vector $\\mathbf{w}_{\\rm opt}$ is stored in the attribute `Ridge.coef_` of the `Ridge` instance. \n",
    "\n",
    "[See documentation of Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef05953e6a07a985",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "m = 20\n",
    "n = 10\n",
    "X, y = GetFeaturesLabels(m, n)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2)  # 80% training and 20% test\n",
    "\n",
    "alpha = 10    # Define value of the regularization parameter 'alpha'\n",
    "\n",
    "ridge = Ridge(alpha=alpha, fit_intercept=True)    # Create Ridge regression model\n",
    "ridge.fit(X_train, y_train)    # Fit the Ridge regression model on the training set\n",
    "y_pred = ridge.predict(X_train)    # Predict the labels of the training set\n",
    "w_opt = ridge.coef_    # Get the optimal weights (regression coefficients) of the fitted model\n",
    "err_train = mean_squared_error(y_pred, y_train)    # Calculate the training error\n",
    "\n",
    "# Print optimal weights and training error\n",
    "print('Optimal weights: \\n', w_opt)\n",
    "print('Training error: \\n', err_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f0f6530942348129",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**The Lasso** is another regularized regression method, which regularizes the training error of linear predictors $h(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$ with the complexity measure $\\mathcal{R}(h)= \\|\\mathbf{w}\\|_{1}$. In Lasso regression, it is customary to use a regularized loss function where the training error term is multiplied by $1/2$. That is,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{E}(\\textbf{w}) = \\frac{1}{2m_t} \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - h(\\mathbf{x}^{(i)}) \\big)^{2} + \\alpha |\\textbf{w}|.\n",
    "\\end{equation}\n",
    "\n",
    "Since $\\alpha$ can be freely chosen, the multiplicative constant is of no practical importance and is only included in order to make analytical calculations more convenient. Still, the exact form of the loss function is important knowledge since this loss function is typically used in Lasso implementations, incuding the one in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b63b8bd46646688d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='lassoReg'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Lasso Regression.\n",
    "\n",
    "Complete the function `fit_lasso` that uses the Scikit-learn function `Lasso.fit()` to compute the optimal predictor for $\\alpha=$ `alpha_val`. When initializing Lasso, please use `fit_intercept=True`. This function is then used find the optimal Lasso predictor for $\\alpha = 10$.\n",
    "\n",
    "[Documentation for Lasso in Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b6cd24ae92c6b545",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X,y = GetFeaturesLabels(m,n)    # read in m data points using n features \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2)    # 80% training and 20% test\n",
    "\n",
    "def fit_lasso(X_train, y_train, alpha_val):\n",
    "    ### STUDENT TASK ###\n",
    "    # .\n",
    "    # .\n",
    "    # .\n",
    "    # w_opt = ...\n",
    "    # training_error = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    lasso = Lasso(alpha=alpha_val, fit_intercept=True)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    y_pred = lasso.predict(X_train)\n",
    "    w_opt = lasso.coef_\n",
    "    training_error = mean_squared_error(y_pred, y_train)\n",
    "    ### END SOLUTION\n",
    "    return w_opt, training_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-44c7f2c09ba11e52",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set alpha value\n",
    "alpha_val = 10\n",
    "\n",
    "# Fit Lasso and calculate optimal weights and training error using the function 'fit_lasso'\n",
    "w_opt, training_error = fit_lasso(X_train, y_train, alpha_val)\n",
    "\n",
    "# Print optimal weights and the corresponding training error\n",
    "print('Optimal weights: \\n', w_opt)\n",
    "print('Training error: \\n', training_error)\n",
    "\n",
    "# Perform some sanity checks on the outputs\n",
    "from sklearn.linear_model import Lasso\n",
    "assert w_opt.reshape(-1,1).shape == (10,1), \"'w_opt' has wrong shape\"\n",
    "assert np.isscalar(training_error), \"'training_error' is not scalar\"\n",
    "assert training_error < 1000, \"'training_error' is too large\"\n",
    "print('Sanity check tests passed!')\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "def t_fitLasso(x_train, y_train, lambd=0):\n",
    "    t_lasso = Lasso(alpha=lambd, fit_intercept=True)\n",
    "    t_lasso.fit(X_train, y_train)\n",
    "    y_pred = t_lasso.predict(X_train)\n",
    "    w_opt = t_lasso.coef_\n",
    "    training_error = mean_squared_error(y_pred, y_train)\n",
    "    return w_opt, training_error\n",
    "\n",
    "t_w_opt, t_training_error = t_fitLasso(X_train, y_train, lambd=10)\n",
    "\n",
    "np.testing.assert_allclose(t_w_opt.reshape(-1,1), w_opt.reshape(-1,1), rtol=1e-10, atol=0)\n",
    "np.testing.assert_almost_equal(t_training_error, training_error)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1355f27ba4fd6ca5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "When using Lasso or ridge regression, we need to find a suitable value for the regularization parameter $\\alpha$. A simple but useful approach is **grid search**: We first specify a list of values to be used for the regularization parameter. For each value $\\alpha$, we determine a predictor $h^{(\\alpha)}$ by minimizing the regularized training error: \n",
    "\\begin{equation}\n",
    "h^{(\\alpha)}  = {\\rm argmin}_{h \\in \\mathcal{H}} (1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - h(\\mathbf{x}^{(i)}) \\big)^{2} + \\alpha \\mathcal{R}(h).    \n",
    "\\end{equation}\n",
    "The resulting training error is \n",
    "\\begin{equation} \n",
    "E_{\\rm train}(\\alpha) = (1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - h^{(\\alpha)}(\\mathbf{x}^{(i)}) \\big)^{2}. \n",
    "\\end{equation}\n",
    "Note that the training error $E_{\\rm train}(\\alpha)$ is measured on the training data $\\mathbb{X}^{t}$ which was also used to tune the predictor $h^{(\\alpha)}$ (in the above opimtization problem). Therefore, $E_{\\rm train}(\\alpha)$ is too optimistic as a measure for the average error of $h^{(\\alpha)}$ on new data points. Instead, we will measure the quality of $h^{(\\alpha)}$ via the validation error  \n",
    "\\begin{equation} \n",
    "E_{\\rm val}(\\alpha) = (1/m_{v}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(v)}} \\big(y^{(i)} - h^{(\\alpha)}(\\mathbf{x}^{(i)}) \\big)^{2} \n",
    "\\end{equation}\n",
    "incurred by the predictor $h^{(\\alpha)}$ on the validation set $\\mathbb{X}^{(v)}$. We then choose the value $\\alpha$ resulting in the smallest validation error $E_{\\rm val}(\\alpha)$. This grid search can be computationally expensive since we have to solve a separate optimization problem (of minimizing the regularized training error) for each value of $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8e4d019532f4bff3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='lassoParameter'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Tuning Lasso Parameter.\n",
    "    \n",
    "Complete the function `lasso_param_search` that computes the Lasso estimator $h^{(\\alpha)}$ for each value $\\alpha$ in the input parameter `alpha_values`. The function returns the resulting validation errors $E_{\\rm val}(\\alpha^{(i)})$ and training errors $E_{\\rm train}(\\alpha^{(i)})$ in the numpy arrays `err_val` of shape (`n_values`,1) and `err_train` of shape (`n_values`,1), as well as the weight vector of the optimal model with the optimal alpha $\\hat{\\alpha}$ (yielding the smallest validation error) in the variable `w_opt`. In the error arrays, the first entry `err_val[0]` should be $E_{\\rm val}(\\alpha^{(1)})$, and so on. \n",
    "\n",
    "[scikit-learn function for Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) \n",
    "\n",
    "\n",
    "<p><b>Hint:</b> Please ignore `ConvergenceWarning`.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef618671a3220a4f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def lasso_param_search(X_train, X_val, y_train, y_val, alpha_values):\n",
    "    n_values = len(alpha_values)    # The number of candidate values for 'alpha'\n",
    "    err_train = np.zeros([n_values,1])    # Array for training errors\n",
    "    err_val = np.zeros([n_values,1])    # Array for validation errors\n",
    "    \n",
    "    ### STUDENT TASK ###\n",
    "    # Pseudocode:\n",
    "    # -For each alpha in alpha_values:\n",
    "    #   -fit a lasso model on the training data\n",
    "    #   -calculate and store training and validation errors\n",
    "    # -Find the best alpha (i.e. the one with the lowest validation error)\n",
    "    # -Calculate/retrieve the optimal weights (coefficients) corresponding to this alpha\n",
    "    ### BEGIN SOLUTION\n",
    "    for l in range(n_values):\n",
    "        lasso = Lasso(alpha=alpha_values[l], fit_intercept=False)\n",
    "        lasso = lasso.fit(X_train, y_train)\n",
    "        y_train_pred = lasso.predict(X_train)\n",
    "        err_train[l] = mean_squared_error(y_train_pred, y_train)\n",
    "        y_val_pred = lasso.predict(X_val)\n",
    "        err_val[l] = mean_squared_error(y_val_pred, y_val)\n",
    "\n",
    "    best_alpha_idx = np.argmin(err_val)\n",
    "    lasso = Lasso(alpha=alpha_values[best_alpha_idx], fit_intercept=False)\n",
    "    lasso = lasso.fit(X_train, y_train)\n",
    "    w_opt = lasso.coef_\n",
    "    ### END SOLUTION\n",
    "    return w_opt, err_train, err_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b3b01a65b2b8c214",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check tests passed!\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nNot equal to tolerance rtol=1e-10, atol=0\n\nMismatch: 100%\nMax absolute difference: 0.34861554\nMax relative difference: 0.10168802\n x: array([[5.255712],\n       [2.821628],\n       [0.632425],...\n y: array([[5.604328],\n       [2.97198 ],\n       [0.57405 ],...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-d48eae44c815>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mt_w_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_err_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_err_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_lasso_param_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_allclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_err_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_allclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_err_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_allclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_w_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\u001b[0m in \u001b[0;36massert_allclose\u001b[1;34m(actual, desired, rtol, atol, equal_nan, err_msg, verbose)\u001b[0m\n\u001b[0;32m   1499\u001b[0m     \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Not equal to tolerance rtol=%g, atol=%g'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),\n\u001b[1;32m-> 1501\u001b[1;33m                          verbose=verbose, header=header, equal_nan=equal_nan)\n\u001b[0m\u001b[0;32m   1502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[1;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[0;32m    825\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[1;32m--> 827\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    828\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=1e-10, atol=0\n\nMismatch: 100%\nMax absolute difference: 0.34861554\nMax relative difference: 0.10168802\n x: array([[5.255712],\n       [2.821628],\n       [0.632425],...\n y: array([[5.604328],\n       [2.97198 ],\n       [0.57405 ],..."
     ]
    }
   ],
   "source": [
    "# Specify a list of values for alpha to be considered\n",
    "alpha_values = np.array([0.01, 0.05, 0.2, 1, 3, 10, 1e2, 1e3])\n",
    "\n",
    "# Calculate the optimal weights, and training and validation errors for the alpha values defined above using lasso_param_search\n",
    "w_opt, err_train, err_val = lasso_param_search(X_train, X_val, y_train, y_val, alpha_values)\n",
    "\n",
    "# Perform some sanity checks on the outputs\n",
    "assert w_opt.reshape(-1,1).shape == (10,1), \"'w_opts' has wrong shape\"\n",
    "assert len(err_train) == 8, \"'err_train' has wrong shape\"\n",
    "assert len(err_val) == 8, \"'err_val' has wrong shape\"\n",
    "print('Sanity check tests passed!')\n",
    "\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "def t_lasso_param_search(X_train, X_val, y_train, y_val, alpha_values):\n",
    "    n_values = len(alpha_values)\n",
    "    err_train = np.zeros([n_values,1]) # Array for training errors\n",
    "    err_val = np.zeros([n_values,1]) # Array for validation errors\n",
    "    \n",
    "    for l in range(n_values):\n",
    "        lasso = Lasso(alpha=alpha_values[l], fit_intercept=True)\n",
    "        lasso = lasso.fit(X_train, y_train)\n",
    "        y_train_pred = lasso.predict(X_train)\n",
    "        err_train[l] = mean_squared_error(y_train_pred, y_train)\n",
    "        y_val_pred = lasso.predict(X_val)\n",
    "        err_val[l] = mean_squared_error(y_val_pred, y_val)\n",
    "\n",
    "    best_alpha_idx = np.argmin(err_val)\n",
    "    lasso = Lasso(alpha=alpha_values[best_alpha_idx], fit_intercept=True)\n",
    "    lasso = lasso.fit(X_train, y_train)\n",
    "    w_opt = lasso.coef_\n",
    "    w_opt = w_opt.reshape(-1,1)\n",
    "    return w_opt, err_train, err_val\n",
    "    \n",
    "t_w_opt, t_err_train, t_err_val = t_lasso_param_search(X_train, X_val, y_train, y_val, alpha_values)\n",
    "    \n",
    "np.testing.assert_allclose(t_err_val.reshape(-1,1), err_val.reshape(-1,1), rtol=1e-10, atol=0)\n",
    "np.testing.assert_allclose(t_err_train.reshape(-1,1), err_train.reshape(-1,1), rtol=1e-10, atol=0)\n",
    "np.testing.assert_allclose(t_w_opt, w_opt.reshape(-1,1), rtol=1e-10, atol=0)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-535f1b0a4667461c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training and validation errors\n",
    "plt.figure(figsize=(10,6))    # Set figure size\n",
    "plt.plot(alpha_values, err_train, marker='o', color='black', label='training error')    # Plot training errors\n",
    "plt.plot(alpha_values, err_val, marker='o', color='red', label='validation error')    # Plot validation errors\n",
    "plt.xscale('log')    # Set x-axis to logarithmic scale\n",
    "plt.xlabel(r'$\\alpha$')    # Set label of x-axis\n",
    "plt.ylabel(r'$E(\\alpha)$')    # Set label of y-axis\n",
    "plt.title(r'Errors with respect to $\\alpha$', fontsize=16)    # Set title\n",
    "plt.legend()    # Show legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-25e8202490edf5af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Take Home Quiz\n",
    "\n",
    "Answer the following questions by setting, for each question, the variable `answer_R4_Q??` to the index of the correct answer. E.g. if you think that the second answer in the first quiz question is the right one, then set `answer_R4_Q1=2`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-30fe04a19dab009c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='QuestionR4_1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<p><b>Student Task.</b> Question R4.1. </p>\n",
    "\n",
    "<p>What is the goal of model selection in machine learning?</p>\n",
    "\n",
    "<ol>\n",
    "  <li> To choose (learn) the optimal predictor function $h_{\\rm opt}$ out of a given hypothesis space (model) $\\mathcal{H}$.</li>\n",
    "  <li> To select the most suitable car model using machine learning methods.</li>\n",
    "  <li> To select the optimal weights used for regularization.</li>\n",
    "  <li> To select the best hypothesis space out of a set of candidates $\\lbrace \\mathcal{H}^{(1)}, \\mathcal{H}^{(2)}, \\ldots,\\mathcal{H}^{(n)} \\rbrace$.</li>\n",
    "</ol> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-01dd62184f9f57d9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer_R4_Q1  = ...\n",
    "### BEGIN SOLUTION\n",
    "answer_R4_Q1 = 4\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b0c15dcdd6f57af7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "assert answer_R4_Q1 in [1,2,3,4], '\"answer_R4_Q1\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert answer_R4_Q1 in [1,2,3,4], '\"answer_R4_Q1\" Value should be an integer between 1 and 4.'\n",
    "assert answer_R4_Q1 == 4, ' \"answer_R4_Q1\" Correct answer is 4.'\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2d849c30b77c3a9d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='QuestionR4_2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<p><b>Student Task.</b> Question R4.2. </p>\n",
    "\n",
    "<p>What is a good measure for the prediction error (loss) incurred by a predictor function $h(\\mathbf{x})$ on new data points?</p>\n",
    "<ol>\n",
    "  <li> The empirical error (average loss) of $h(\\mathbf{x})$ on the <b>training set</b> which is also used to tune $h(\\mathbf{x})$. </li>\n",
    "  <li> The empirical error (average loss) of $h(\\mathbf{x})$ on some <b>validation set</b> which is different from the training set. \n",
    "</ol> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-786b53bfbed054d1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer_R4_Q2  = ...\n",
    "### BEGIN SOLUTION\n",
    "answer_R4_Q2 = 2\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fcd79fc73255e27d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "assert answer_R4_Q2 in [1,2], '\"answer_R4_Q2\" Value should be an integer between 1 and 2.'\n",
    "print('Sanity check tests passed!')\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert answer_R4_Q2 in [1,2], '\"answer_R4_Q2\" Value should be an integer between 1 and 2.'\n",
    "assert answer_R4_Q2 == 2, ' \"answer_R4_Q2\" Correct answer is 2.'\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e89debc2a73facf2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='QuestionR4_3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<p><b>Student Task.</b> Question R4.3. </p>\n",
    "\n",
    "Regularized linear regression amounts to finding the predictor $h(\\mathbf{x})$ which minimizes the regularized training error \n",
    "\\begin{equation} \n",
    "(1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - h(\\mathbf{x}^{(i)}) \\big)^{2} + \\alpha \\mathcal{R}(h).\n",
    "\\end{equation}\n",
    "Which statement is true?\n",
    "\n",
    "<ol>\n",
    "  <li> Using a large value for the regularization parameter $\\alpha$ prefers predictors with large complexity $\\mathcal{R}(h)$ but small training error.</li>\n",
    "  <li>  Using a small value for the regularization parameter $\\alpha$ prefers predictors with large complexity $\\mathcal{R}(h)$ but small training error.</li>\n",
    "  <li> For regularization parameter $\\alpha=0$, the optimal predictor is always $h(\\mathbf{x}) =0$. </li>\n",
    "  <li> For regularization parameter $\\alpha=0$, the optimal predictor is always $h(\\mathbf{x}) =42$.</li>\n",
    "</ol> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a7f812f9a71b8f3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer_R4_Q3  = ...\n",
    "### BEGIN SOLUTION\n",
    "answer_R4_Q3 = 2\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-530e6746dd4af06c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "assert answer_R4_Q3 in [1,2,3,4], '\"answer_R4_Q3\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert answer_R4_Q3 in [1,2,3,4], '\"answer_R4_Q3\" Value should be an integer between 1 and 4.'\n",
    "assert answer_R4_Q3 == 2, ' \"answer_R4_Q3\" Correct answer is 2.'\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2bd3fa9a8a8b2da9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='QuestionR4_4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<p><b>Student Task.</b> Question R4.4. </p>\n",
    "\n",
    "Use the previously implemented code in \"Tuning Lasso Parameter\" to find the optimal predictor (lowest validation error) for $\\alpha=$ `alpha_val`. Using the same dataset.\n",
    "When initializing Lasso, please use `fit_intercept=True`. \n",
    "<p>Which alpha should be chosen to achieve an optimal predictor?</p> \n",
    "<p>Possible alpha values are given in the code cell below.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-11392734fc803cfe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "alpha_values = np.array([0.01, 0.05, 0.2, 1, 3, 10, 1e2, 1e3])\n",
    "\n",
    "# answer_R4_Q4  = ...\n",
    "### BEGIN SOLUTION\n",
    "answer_R4_Q4 = 3\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d7f48b684296863d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "assert answer_R4_Q4 in [0.01, 0.05, 0.2, 1, 3, 10, 1e2, 1e3], 'answer_R4_Q3\" Value should be a value out of given alpha values..'\n",
    "print('Sanity check tests passed!')\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert answer_R4_Q4 in [0.01, 0.05, 0.2, 1, 3, 10, 1e2, 1e3], 'answer_R4_Q4\" Value should be an integer between 1 and 4.'\n",
    "assert answer_R4_Q4 == 3, ' answer_R4_Q4\" Correct answer is 3.'\n",
    "### END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
